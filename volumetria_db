from pyspark.sql import Row
from pyspark.sql.functions import col, sum as _sum

# bases e dias que você quer analisar
databases = ["db1", "db2"]
dias = ["2025-08-01", "2025-08-02"]

history_results = []
detail_results = []

for database in databases:
    tables = [t.name for t in spark.catalog.listTables(database)]
    
    for table in tables:
        for dia in dias:
            table_name = f"{database}.{table}"

            try:
                # --- PRIMEIRA TENTATIVA: DESCRIBE HISTORY ---
                history = spark.sql(f"DESCRIBE HISTORY {table_name}") \
                    .filter(col("timestamp").cast("date") == dia)

                if "operationMetrics" in history.columns:
                    row = history.select("operationMetrics").orderBy(col("version").desc()).first()
                    if row and row.operationMetrics:
                        numRows = int(row.operationMetrics.get("numOutputRows", -1))
                        numBytes = int(row.operationMetrics.get("numOutputBytes", -1))
                        history_results.append(Row(database=database, table=table, dia=dia,
                                                   numRows=numRows, sizeInBytes=numBytes))
                        continue  # já pegou do history, pula para próxima

                # se chegou aqui, não encontrou no history
                raise Exception("Sem métricas no history")

            except Exception as e:
                try:
                    # --- FALLBACK: DESCRIBE DETAIL + COUNT ---
                    detail = spark.sql(f"DESCRIBE DETAIL {table_name}").first()
                    sizeInBytes = int(detail.sizeInBytes) if detail and detail.sizeInBytes else -1

                    # identificar partição
                    partition_cols = detail.partitionColumns if detail else []
                    if partition_cols:
                        partition_col = partition_cols[0]  # assume só 1 partição
                        numRows = spark.table(table_name).filter(col(partition_col) == dia).count()
                    else:
                        # se não tem partição, faz count completo (cuidado com tabelas grandes!)
                        numRows = spark.table(table_name).count()

                    detail_results.append(Row(database=database, table=table, dia=dia,
                                              numRows=numRows, sizeInBytes=sizeInBytes))

                except Exception as e2:
                    # fallback final → erro
                    detail_results.append(Row(database=database, table=table, dia=dia,
                                              numRows=-1, sizeInBytes=-1))

# --- criar dataframes separados ---
history_df = spark.createDataFrame(history_results)
detail_df = spark.createDataFrame(detail_results)

# --- agrupar por database ---
history_agg = history_df.groupBy("database", "dia") \
    .agg(_sum("numRows").alias("total_rows"),
         _sum("sizeInBytes").alias("total_size_bytes"))

detail_agg = detail_df.groupBy("database", "dia") \
    .agg(_sum("numRows").alias("total_rows"),
         _sum("sizeInBytes").alias("total_size_bytes"))
