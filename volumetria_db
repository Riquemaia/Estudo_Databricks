from pyspark.sql import Row
from pyspark.sql.functions import col, sum as _sum

# bases e dias que vocÃª quer analisar
databases = ["db1", "db2"]
dias = ["2025-08-01", "2025-08-02"]

results = []

for db in databases:
    tables = spark.catalog.listTables(db)
    for t in tables:
        full_table = f"{db}.{t.name}"

        for dia in dias:

            # 1) history
            try:
                hist = spark.sql(f"DESCRIBE HISTORY {full_table}")
                hist_filtered = (
                    hist
                    .filter(F.col("timestamp").cast("date") == F.lit(dia))
                    .select("operationMetrics", "timestamp")
                    .limit(1)
                ).collect()

                if hist_filtered:
                    op = hist_filtered[0].operationMetrics.asDict()
                    results.append(
                        (db, t.name, dia,
                         int(op.get("numOutputRows", 0)),
                         int(op.get("numOutputBytes", 0)),
                         "history")
                    )
            except:
                pass

            # 2) detail
            try:
                detail = spark.sql(f"DESCRIBE DETAIL {full_table}").collect()[0]
                size_in_bytes = detail.sizeInBytes or 0
                num_files = detail.numFiles or 1
                avg_size = size_in_bytes // num_files
                num_rows = detail.numRows or 0

                results.append(
                    (db, t.name, dia,
                     num_rows,
                     avg_size,
                     "detail")
                )
            except:
                pass

            # 3) count sempre
            try:
                partitions = spark.sql(f"SHOW PARTITIONS {full_table}").collect()
                part_col = None
                if partitions:
                    part_col = partitions[0][0].split("=")[0]

                if part_col:
                    cnt = spark.sql(f"SELECT count(*) as c FROM {full_table} WHERE {part_col} = '{dia}'").collect()[0].c
                else:
                    cnt = spark.sql(f"SELECT count(*) as c FROM {full_table}").collect()[0].c

                results.append((db, t.name, dia, cnt, 0, "count"))
            except:
                pass

# cria DF final
df = spark.createDataFrame(results, ["database", "table", "dia", "num_rows", "size_bytes", "source"])

# salva em tabela particionada Delta
(
    df.write
    .format("delta")
    .mode("append")
    .partitionBy("database", "table", "dia")
    .saveAsTable("monitoring.table_metrics")
)
